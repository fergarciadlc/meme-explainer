Example output

```
****************************************************************************************************
Testing for GPT-3.5-turbo
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
Ah, this one's a classic in the programming world! You see, Python and C# are both programming languages, but they have different syntax and features. The joke plays on the fact that the Python programmer got cold during winter because they didn't "C sharp," which sounds like "see sharp," a reference to the C# programming language. So, the humor comes from the clever play on words between "see sharp" and staying warm during winter. It's a light-hearted jab at programmers and their choice of programming languages, adding a bit of tech-related humor to the mix! ü§ìüß£
****************************************************************************************************
Testing for GPT-4o IMAGE
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
Ah, the classic "gradient ascent" joke! To understand why this is funny, you need a little bit of background in machine learning jargon.

In machine learning, "gradient descent" is a common algorithm used for optimizing models by iteratively moving towards the minimum of a cost function‚Äîhence, "descent." It's all about minimizing errors or losses in your model.

This meme, however, refers to "gradient ascent," which would imply the algorithm is moving towards maximizing something. In the context of the meme, you can see that the "loss" values are continuously increasing, which is the opposite of what you want in most scenarios. It's like saying, "Hey, we aimed to reduce errors, but instead, our errors are climbing Mt. Everest."

So the humor lies in the ironic twist: everyone knows they should be achieving gradient descent, but instead, things have gone hilariously wrong, and they're achieving something entirely counterproductive‚Äîgradient ascent. It‚Äôs a programmer‚Äôs way of poking fun at how sometimes things don‚Äôt just fail‚Äîthey fail spectacularly.
```



V2
```
****************************************************************************************************
Testing for GPT-3.5-turbo
INFO:__main__:Sending request to model: gpt-3.5-turbo
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
Ah, classic tech humor right there! So, let me break it down for you. In this joke, it's playing on the names of two popular programming languages: Python and C#. The Python programmer got cold during winter because they didn't "C#" ‚Äì get it? In programming lingo, the "#" symbol is pronounced as "sharp," so it sounds like "see sharp." The joke is a pun on the idea that the Python programmer didn't "C sharp," implying they didn't see or notice the cold coming, which is why they got cold. It's a playful way of blending coding languages with a common everyday situation, making it both clever and chuckle-worthy. Plus, the snake emoji üêç for Python adds a nice touch of visual humor to the mix.
****************************************************************************************************
Testing for GPT-4o IMAGE
INFO:__main__:Sending request to model: gpt-4o
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
Alright, buckle up for this one! So, the tweet says "gradient ascent has been achieved internally" and shows a bunch of loss values from, presumably, a machine learning model's training process.

Here's the deal: In machine learning, "gradient descent" is a common optimization algorithm used to minimize loss (basically, make the model's predictions closer to reality). It‚Äôs called "descent" because we want to go downhill on the loss function to find the minimum loss value.

Now, the tweet hilariously mentions "gradient ascent," which is essentially the opposite, meaning our model is heading uphill, making things worse, not better. 

The punchline? The screenshot shows values with increasing losses instead of decreasing. So, instead of getting better at predicting, the model's getting worse. It‚Äôs like if you hired a personal trainer to get you fit, but suddenly you gained 20 pounds instead. Like saying: "Congratulations, we‚Äôre achieving the opposite of what we intended!"

Classic tech humor right there. üèãÔ∏è‚Äç‚ôÇÔ∏èüìà
```
